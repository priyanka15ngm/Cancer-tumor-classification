# -*- coding: utf-8 -*-
"""cancer_tumor_classification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1U7cgf823l6VNGV-aV_DpvU2uZq62i_wh
"""

import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib as plt

!pip3 install pandas --upgrade

from sklearn.datasets import load_breast_cancer
df = load_breast_cancer()

print(df)

df.keys()

df['target']

df=pd.DataFrame(np.c_[df['data'],df['target']],
             columns = np.append(df['feature_names'], ['target']))

df.head(10)

df.isnull().values.any()

df.keys()

df.describe()

df.info()

#count of no.of patient with cancer or not
df['target'].value_counts()

#visualize the count
sns.countplot(df['target'])

#getting the correlation of columns
df.corr()

#visualize the data
import matplotlib.pyplot as plt

plt.figure(figsize=(20,20))
sns.heatmap(df.corr(),annot= True,fmt = '.0%')

sns.pairplot(df, hue = 'target', 
             vars = ['mean radius', 'mean texture', 'mean perimeter', 'mean area', 'mean smoothness'] )

feature_names=['mean radius', 'mean texture', 'mean perimeter', 'mean area',
       'mean smoothness', 'mean compactness', 'mean concavity',
       'mean concave points', 'mean symmetry', 'mean fractal dimension',
       'radius error', 'texture error', 'perimeter error', 'area error',
       'smoothness error', 'compactness error', 'concavity error',
       'concave points error', 'symmetry error', 'fractal dimension error',
       'worst radius', 'worst texture', 'worst perimeter', 'worst area',
       'worst smoothness', 'worst compactness', 'worst concavity',
       'worst concave points', 'worst symmetry', 'worst fractal dimension']
x =df[feature_names]

x

y = df.target

y

from sklearn.model_selection import train_test_split

train_x, test_x, train_y, test_y = train_test_split(x, y, random_state = 0)

#feature scaling
#scale down the values between 0 and 1
 
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
train_x = sc.fit_transform(train_x)
test_x = sc.transform(test_x)





"""Logistic Regression"""



from sklearn.linear_model import LogisticRegression
log = LogisticRegression(random_state = 0)
log.fit(train_x,train_y)

# evaluating the model accuracy on training dataset
model_1 = log
model_1.score(train_x,train_y)

#error rate = 1-accuracy
# evaluating the model_1 error rate on training dataset
e1 = 1 - 0.9906103286384976
e1

preds_m1 = model_1.predict(test_x)

#evaluating the model accuracy on testing dataset
from sklearn.metrics import confusion_matrix

cm_m1 = confusion_matrix(test_y,preds_m1)
print(cm_m1)

#evaluating the models accuracy on the test data set

TN = cm_m1[0][0]
TP = cm_m1[1][1]
FN = cm_m1[1][0]
FP = cm_m1[0][1]

acc_m1 = ((TP+TN)/(TP+TN+FN+FP))

print('Model_1 Test Accuracy = {}'.format(acc_m1))

e11 = 1-0.958041958041958
print('Model_1 Test error rate : {}'.format(e11))

#using another method to calculate error rate

e11_m1 = ((FP+FN)/(TP+TN+FN+FP))
print('Model_1 Test error rate : {}'.format(e11_m1))

#also we can directly calculte the models accuracy 

from sklearn.metrics import  accuracy_score

print('model_1 accuracy is ',end ='')
accuracy_score(test_y,preds_m1)

print('model_1 error rate on testing dataset is ',end ='')
e2 = 1 -0.958041958041958
e2



"""Decision Tree Classifier"""

from sklearn.tree import DecisionTreeClassifier
tree = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)
tree.fit(train_x, train_y)

# testing the model_2 accuracy on training dataset
model_2 = tree
model_2.score(train_x,train_y)

print('model_2 error rate on training dataset')
e2 = 1 - 1.0
e2

preds_m2 = model_2.predict(test_x)

#confusion matrix for model_2
cm_m2 = confusion_matrix(test_y,preds_m2)
print(cm_m2)

#printing model_2 accuracy
print("model_2 accuracy on testing dataset ")
accuracy_score(test_y,preds_m2)

print('model_2 error rate on testing dataset',end = ' ')
e22 = 1 - 0.958041958041958
e22

"""Random Forest Classifier"""

from sklearn.ensemble import RandomForestClassifier
forest = RandomForestClassifier(n_estimators = 10 , criterion = 'entropy',random_state = 0)
forest.fit(train_x,train_y)

# testing the model3 accuracy on training dataset
model_3 = forest
model_3.score(train_x,train_y)

print('model_3 error rate on training dataset is ',end = '')
e3 = 1 - 1.0
e3

preds_m3 = model_3.predict(test_x)

#confusion matrix for model_3
cm_m3 = confusion_matrix(test_y,preds_m3)
print(cm_m3)

#printing model_3 accuracy
print("model_3 accuracy on testing dataset is ")
accuracy_score(test_y,preds_m3)

print("model_3 error rate on testing dataset is ")
e33 = 1-0.972027972027972
e33



"""Knn classifier"""

from sklearn.neighbors import KNeighborsClassifier
knn = KNeighborsClassifier(n_neighbors = 5)
knn.fit(train_x, train_y)

#Evaluating the model accuracy on training dataset
model_4 = knn
model_4.score(train_x, train_y)

#error rate = 1 - accuracy
e4 = 1 - 0.9741784037558685
e4

preds_m4 = model_4.predict(test_x)

#confusion matrix for model_4
cm_m4 = confusion_matrix(test_y,preds_m4)
print(cm_m4)

#printing model_4 accuracy
print("model_4 accuracy on testing dataset is ")
accuracy_score(test_y,preds_m4)

print("model_4 error rate on testing dataset is ")
e44 = 1-0.951048951048951
e44

"""So,if we compare above four models then we see that MODEL_3 which is based on Random Forest Classifier have the largest accuracy of prediction on the testig data"""

#print the prediction of Random Forest Classifier
print(preds_m3)

#print the actual values
print(test_y)

type(test_y)
type(preds_m3)

print(test_y.to_numpy())
print()
print(preds_m3)

#difference between training accuracy and testing accuracy of the models(training accuracy - testing accuracy)

m111=0.0325683705965396

print("model1: {}".format(m111))
m222=0.041958041958042
print("model2: {}".format(m222))

m333=0.027972027972028
print("model3: {}".format(m333))

m444= 0.0231294527069175
print("model4: {}".format(m444))

"""we conclude that Random Forest classifier model (model3) have less chance of overfitting and it is giving the highest accuracy."""



